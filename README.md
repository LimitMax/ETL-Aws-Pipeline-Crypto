# AWS Crypto Data Pipeline

This project demonstrates an **industry-grade end-to-end data engineering pipeline on AWS**
using a **Bronzeâ€“Silverâ€“Gold data lake architecture**.

The pipeline ingests cryptocurrency market data, processes it in batch, and exposes
business-ready datasets for analytics using Amazon Athena.

---

## ðŸ—ï¸ Architecture Overview
EventBridge (Hourly)
â†“
AWS Lambda (Bronze Ingestion)
â†“
Amazon S3 (Bronze - Raw Events)
â†“
Batch Processing (Silver)
â†“
Amazon S3 (Silver - Parquet, Partitioned)
â†“
Daily Aggregation (Gold)
â†“
Amazon S3 (Gold - Parquet, Business-Ready)
â†“
Amazon Athena (Analytics & Query)


---

## ðŸ”§ Technology Stack

- **AWS Lambda** â€“ Event-driven bronze ingestion
- **Amazon EventBridge** â€“ Hourly scheduling
- **Amazon S3** â€“ Data lake storage (bronze / silver / gold)
- **Amazon Athena** â€“ Ad-hoc analytics on Parquet data
- **Python** â€“ ETL logic (batch processing)
- **Pandas / PyArrow** â€“ Parquet generation and aggregation
- **IAM Roles & OIDC** â€“ Secure authentication (no credentials in code)
- **GitHub Actions** â€“ CI/CD for Lambda deployment

---

## ðŸ¥‰ Bronze Layer (Raw Ingestion)

**Purpose**
- Capture ingestion events and metadata
- Lightweight, append-only, no transformation

**Characteristics**
- Triggered hourly via EventBridge
- Implemented using AWS Lambda
- Stores raw ingestion metadata in S3

**Design Choice**
> Lambda is intentionally limited to lightweight ingestion to avoid
dependency size limits and long execution times.

---

## ðŸ¥ˆ Silver Layer (Clean & Structured Data)

**Purpose**
- Transform raw ingestion events into structured OHLCV market data
- Enforce schema consistency and deduplication

**Key Features**
- Batch processing (not event-driven)
- OHLCV data at **hourly granularity**
- Stored as **Parquet with partitioning**:

- Optimized for analytics and downstream processing

**Why Batch?**
> Batch processing is more cost-efficient and easier to manage for
transformations and deduplication compared to per-event execution.

---

## ðŸ¥‡ Gold Layer (Business-Ready Aggregation)

**Purpose**
- Provide analytics-ready, business-focused datasets

**Current Gold Dataset**
- **Daily OHLCV aggregation per symbol**

**Gold Characteristics**
- Aggregated from Silver (never from Bronze)
- Stored as **partitioned Parquet**
- Optimized for reporting and dashboards

---

## ðŸ“Š Analytics Layer

- **Amazon Athena** is used to query both Silver and Gold datasets
- Separate Athena databases are used per layer:
- `crypto_analytics` â†’ Silver (technical layer)
- `crypto_gold` â†’ Gold (business layer)

This separation ensures clear data ownership and prevents mixing
technical and business concerns.

---

## ðŸ” Security & Best Practices

- No AWS credentials stored in code or repository
- IAM Role-based access for AWS services
- Local development uses IAM users with least-privilege access
- Clear separation between ingestion, processing, and analytics layers

---

## ðŸš€ Current Status

- âœ… AWS account, budget, and IAM (least privilege) configured
- âœ… S3 data lake (bronze / silver / gold) implemented
- âœ… Bronze ingestion Lambda deployed via CI/CD
- âœ… Silver batch processing implemented (Parquet + partitioned)
- âœ… Gold daily aggregation implemented (Parquet + partitioned)
- âœ… Athena successfully querying Silver and Gold datasets
- â³ AWS Glue Data Catalog & scheduled Glue Jobs (planned)

---

## ðŸ“Œ Future Enhancements

- Register Silver & Gold datasets in **AWS Glue Data Catalog**
- Run Silver & Gold pipelines as **scheduled Glue Jobs**
- Add data quality checks and monitoring
- Integrate BI visualization (e.g. QuickSight)

---

## ðŸ§  Key Takeaway

> This project focuses on **realistic, production-oriented design decisions**
rather than forcing all logic into serverless functions, demonstrating
how modern data platforms are built and operated in practice.
